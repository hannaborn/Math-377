{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "from math import *\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 24: Hypothesis Testing Errors & Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this block, we have been studying hypothesis tests. We have covered the four basic steps of any hypothesis test, and we have practiced various methods for obtaining the distribution of our test statistic under the null hypothesis. \n",
    "\n",
    "After we have reached a conclusion (reject or fail to reject), we must consider possible errors. \n",
    "\n",
    "### Type I error \n",
    "\n",
    "Type I error is the event that we rejected the null hypothesis when the null hypothesis was actually true. Type I error is also known as a false positive. The probability of a Type I error is usually defined by the threshold used for rejection. A common threshold is 0.05. Those of you who have taken statistics before may recognize this value as $\\alpha$. \n",
    "\n",
    "### Type II error\n",
    "\n",
    "Type II error is the event that we failed to reject the null hypothesis when the null hypothesis was actually false. This is otherwise known as a false negative. The probability of a Type II error is harder to find and requires a more in-depth analysis of a hypothesis test. The probability of a Type II error is often given as $\\beta$, and $1-\\beta$ is referred to as **Power**. The power of a test is probability that we will reject the null hypothesis when we are supposed to. \n",
    "\n",
    "Which one of these errors is more serious? It depends on the context of the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Golf Balls\n",
    "\n",
    "Joe has a summer job at a golf course and one of his jobs is to fish out golf balls from the water traps. He has a theory that certain types of golf ball are more likely to end up in the water than others. Let's assume there are four brands of golf ball, let's and assume that all four are used equally at this golf course. He fishes out 100 golf balls and counts each brand. He finds 30 of brand A, 30 of brand B, 20 of brand C and 20 of brand D. Conduct a hypothesis test to determine whether certain types of golf ball are more likely than others to end up in the water."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Hypotheses\n",
    "\n",
    "H0: P(a) = P(b) = P(c) = P(d)  \n",
    "Ha: at least one P is different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Test statistic\n",
    "\n",
    "There are many correct answers, but let's go with sum of absolute difference between observed and expected counts under $H_0$. To do this, we need to find the expected counts. If each ball was equally likely, how many should we expected to find of each if we select 100 golf balls? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected value = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: $p$-value\n",
    "\n",
    "We need the distribution of the test statistic under $H_0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations  = make_array(30,30,20,20)\n",
    "expected = make_array(25,25,25,25)\n",
    "test_stat = sum(abs(observations-expected))\n",
    "test_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1868"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test statistic distribution\n",
    "ts_dist = []\n",
    "for i in np.arange(10000): \n",
    "    #simulation\n",
    "    rs = stats.multinomial.rvs(100,(0.25,0.25,0.25,0.25),size=1)\n",
    "    ts_dist = np.append(ts_dist,sum(abs(sum(rs)-25)))\n",
    "# get p-value \n",
    "# ... probability that we get data equal to or more extreme than our sample assuming the null is true\n",
    "np.mean(ts>=test_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Conclude\n",
    "\n",
    "0.1868\n",
    "the p-value is high as compared to an alpha of .05 so there insufficient evidence against our null hypothesis therefore we fail to reject the null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of error could we have made in this case? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have made a Type II(false negative) error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power \n",
    "Suppose that, in truth, 30% of the balls found in the water were brand A, 30% were brand B, 20% were brand C and 20% were brand D. In this case, our collected sample reflected this truth perfectly. However, our hypothesis test failed to recognize this deviation from equal proportions. We made a type II error. This is because this test has fairly low power. Use simulation to determine the power of this test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am looking for the probability that I reject the null hypothesis given the true proportions laid out above. Well, first I need to figure out for what values of my test statistic I would reject $H_0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# figure out for what values reject null\n",
    "crit_value = percentile(95,ts_dist)\n",
    "crit_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I need to simulate from the true population and determine how often my test statistic would have met this threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4394"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power = []\n",
    "for i in np.arange(10000):\n",
    "    rs = stats.multinomial.rvs(100,(.3,.3,.2,.2),size=1)\n",
    "    power = np.append(power,sum(abs(sum(rs)-25)))\n",
    "power\n",
    "    # get p-value\n",
    "np.mean(power>=crit_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about this power? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a very reliable power. The test will only identify a false hypothesis roughly 43% of the time even though it was given to be false above. This may be due to the relatively small sample size of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this power calculation, but assume Joe collects 500 balls instead of 100. Note that you will have to obtain a new critical value. What does this tell you about power and sample size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0617"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test statistic distribution\n",
    "ts500_dist = []\n",
    "for i in np.arange(10000): \n",
    "    #simulation\n",
    "    rs = stats.multinomial.rvs(500,(0.25,0.25,0.25,0.25),size=1)\n",
    "    ts500_dist = np.append(ts500_dist,sum(abs(sum(rs)-(5*25))))\n",
    "# get p-value \n",
    "# ... probability that we get data equal to or more extreme than our sample assuming the null is true\n",
    "crit500 = percentile(95, ts500_dist)\n",
    "np.mean(ts500_dist>=crit500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power = 1 - Type II error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.986"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_500 = []\n",
    "for i in np.arange(10000):\n",
    "    rs = stats.multinomial.rvs(500,(.3,.3,.2,.2),size=1)\n",
    "    power_500 = np.append(power_500,sum(abs(sum(rs)-(25*5))))\n",
    "power_500\n",
    "    # get power\n",
    "np.mean(power_500>=crit500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power is a lot higher now. Power is directly related to sample size; power increases with an increase in sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
